version: "3.9"

services:
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    hostname: anythingllm
    container_name: anythingllm
    ports: 
      - 3001:3001
    environment:
      - STORAGE_DIR=/app/server/storage
      - DISABLE_TELEMETRY=true
    depends_on:
      - ollama
    volumes:
      - /storage:/app/server/storage # Recommended to persist data, change left side only (/storage) make sure read and write permissions are set.
    restart: unless-stopped

  jobcollector:
    build:
      context: ../.
      dockerfile: Dockerfile
    image: drjp81/linkedinscrape:latest
    container_name: jobcollector
    environment:
      # Collector parameters, notmally taken from .env file, see example file
      - SITE_NAME=${SITE_NAME:-indeed,linkedin,glassdoor}
      - SEARCH_TERMS=${SEARCH_TERMS:-Azure,devops}
      - LOCATION=${LOCATION:-Canada}
      - RESULTS_WANTED=${RESULTS_WANTED:-20}
      - HOURS_OLD=${HOURS_OLD:-24}
      - COUNTRY_INDEED=${COUNTRY_INDEED:-canada}
      - LINKEDIN_FETCH_DESCRIPTION=${LINKEDIN_FETCH_DESCRIPTION:-true}
      - DATA_DIR=/DATA
      # Evaluator parameters
      - ANLLM_API_KEY=${ANLLM_API_KEY}
      - ANLLM_API_WORKSPACE=${ANLLM_API_WORKSPACE:-job-searching}
      - ANLLM_API_BASE=${ANLLM_API_BASE:-http://anythingllm:3001}
      - RETRIES=${RETRIES:-3}
      - TIMEOUT_MINUTES=${TIMEOUT_MINUTES:-5}
      - CANDIDATE_NAME=${CANDIDATE_NAME:-John Doe}
      # in case you use the ollama only flow
      - OLLAMA_BASE=${OLLAMA_BASE:-http://ollama:11434}
      - SCORER_MODEL=${SCORER_MODEL:-qwen2.5:7b-instruct}
      - EMBED_MODEL=${EMBED_MODEL:-nomic-embed-text}
      - CANDIDATE_RESUME_PATH=${CANDIDATE_RESUME_PATH:-/DATA/resume.txt}
      - MUST_HAVES=${MUST_HAVES}
      - NICE_TO_HAVES=${NICE_TO_HAVES}
      - EXCLUSIONS=${EXCLUSIONS}
      - LOCALE=${LOCALE}
      - LANG_PREF=${LANG_PREF}
      - SENIORITY_TARGET=${SENIORITY_TARGET}
      - K_RESUME_SNIPPETS=${K_RESUME_SNIPPETS:-6}
      - RESUME_CHUNK_SIZE=${RESUME_CHUNK_SIZE:-900}
      - RESUME_CHUNK_OVERLAP=${RESUME_CHUNK_OVERLAP:-150}
    
    volumes:
      - /home/drjpuser/DATA/:/DATA #adjust to persist and make it available locally.
    depends_on:
      - ollama
    restart: on-failure
    #command: ["python3", "-u", "/app/collector.py"]

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    hostname: ollama
    # Ports
    ports:
      - 11434:11434
    # Environment variables
    environment:
      #- OLLAMA_DEBUG=2
      - OLLAMA_ORIGINS=*
    runtime: nvidia
    privileged: true
    network_mode: bridge
    volumes:
      - /mnt/pool3p/dockerdata/ollama/:/root/.ollama
    restart: always
    devices:
      - /dev/dri:/dev/dri
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    healthcheck:
      interval: 1m30s
      timeout: 10s
      retries: 3
      test:  ["CMD", "ollama", "list"]