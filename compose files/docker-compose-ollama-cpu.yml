# Ollama Only - CPU
# Use this if you want local, private LLM inference without GPU
# Requirements: 8+ CPU cores, 16GB+ RAM (slower, but works)
# Recommended: Use smaller models (3B parameters max)

services:
  jobcollector:
    build:
      context: ../.
      dockerfile: Dockerfile
    image: drjp81/linkedinscrape:latest
    container_name: jobcollector
    environment:
      # Collector parameters
      - SITE_NAME=${SITE_NAME:-indeed,linkedin,glassdoor}
      - SEARCH_TERMS=${SEARCH_TERMS:-Azure,devops}
      - LOCATION=${LOCATION:-Canada}
      - RESULTS_WANTED=${RESULTS_WANTED:-20}
      - HOURS_OLD=${HOURS_OLD:-24}
      - COUNTRY_INDEED=${COUNTRY_INDEED:-canada}
      - LINKEDIN_FETCH_DESCRIPTION=${LINKEDIN_FETCH_DESCRIPTION:-true}
      - DATA_DIR=/DATA
      # Ollama parameters (AnythingLLM disabled)
      - OLLAMA_BASE=${OLLAMA_BASE:-http://ollama:11434}
      - SCORER_MODEL=${SCORER_MODEL:-llama3.2:3b-instruct-q6_K}  # Smaller model for CPU
      # - EMBED_MODEL=${EMBED_MODEL:-nomic-embed-text:v1.5}
      - CANDIDATE_NAME=${CANDIDATE_NAME:-John Doe}
      - CANDIDATE_RESUME_PATH=${CANDIDATE_RESUME_PATH:-/DATA/resume/resume.txt}
      - TEMPERATURE=${TEMPERATURE:-0.3}
      - CONTEXT_LENGTH=${CONTEXT_LENGTH:-8192}  # Reduced for CPU
      - MUST_HAVES=${MUST_HAVES}
      - NICE_TO_HAVES=${NICE_TO_HAVES}
      - EXCLUSIONS=${EXCLUSIONS}
      - LOCALE=${LOCALE}
      - LANG_PREF=${LANG_PREF:-either}
      - SENIORITY_TARGET=${SENIORITY_TARGET:-senior}
      - K_RESUME_SNIPPETS=${K_RESUME_SNIPPETS:-6}
      - RESUME_CHUNK_SIZE=${RESUME_CHUNK_SIZE:-900}
      - RESUME_CHUNK_OVERLAP=${RESUME_CHUNK_OVERLAP:-150}
      # Disable AnythingLLM
      - ANLLM_API_KEY=
      - ANLLM_API_WORKSPACE=
      - ANLLM_API_BASE=
    volumes:
      - ${DATA_DIR:-./data}:/DATA
    depends_on:
      ollama:
        condition: service_healthy
    restart: on-failure
    networks:
      - app_network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    hostname: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_DEBUG=0
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=1  # Limit parallelism on CPU
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      - ${OLLAMA_DATA_DIR:-./ollama-data}:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - app_network

networks:
  app_network:
    driver: bridge