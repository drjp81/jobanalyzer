# Ollama Only - AMD GPU
# Use this if you want local, private LLM inference with AMD GPU (ROCm)
# Requirements: AMD GPU with ROCm support (RX 6000/7000 series or newer)

services:
  jobcollector:
    build:
      context: ../.
      dockerfile: Dockerfile
    image: drjp81/linkedinscrape:latest
    container_name: jobcollector
    environment:
      # Collector parameters
      - SITE_NAME=${SITE_NAME:-indeed,linkedin,glassdoor}
      - SEARCH_TERMS=${SEARCH_TERMS:-Azure,devops}
      - LOCATION=${LOCATION:-Canada}
      - RESULTS_WANTED=${RESULTS_WANTED:-20}
      - HOURS_OLD=${HOURS_OLD:-24}
      - COUNTRY_INDEED=${COUNTRY_INDEED:-canada}
      - LINKEDIN_FETCH_DESCRIPTION=${LINKEDIN_FETCH_DESCRIPTION:-true}
      - DATA_DIR=/DATA
      # Ollama parameters (AnythingLLM disabled)
      - OLLAMA_BASE=${OLLAMA_BASE:-http://ollama:11434}
      - SCORER_MODEL=${SCORER_MODEL:-qwen2.5:7b-instruct}
      # - EMBED_MODEL=${EMBED_MODEL:-nomic-embed-text:v1.5}
      - CANDIDATE_NAME=${CANDIDATE_NAME:-John Doe}
      - CANDIDATE_RESUME_PATH=${CANDIDATE_RESUME_PATH:-/DATA/resume/resume.txt}
      - TEMPERATURE=${TEMPERATURE:-0.3}
      - CONTEXT_LENGTH=${CONTEXT_LENGTH:-16384}
      - MUST_HAVES=${MUST_HAVES}
      - NICE_TO_HAVES=${NICE_TO_HAVES}
      - EXCLUSIONS=${EXCLUSIONS}
      - LOCALE=${LOCALE}
      - LANG_PREF=${LANG_PREF:-either}
      - SENIORITY_TARGET=${SENIORITY_TARGET:-senior}
      - K_RESUME_SNIPPETS=${K_RESUME_SNIPPETS:-6}
      - RESUME_CHUNK_SIZE=${RESUME_CHUNK_SIZE:-900}
      - RESUME_CHUNK_OVERLAP=${RESUME_CHUNK_OVERLAP:-150}
      # Disable AnythingLLM
      - ANLLM_API_KEY=
      - ANLLM_API_WORKSPACE=
      - ANLLM_API_BASE=
    volumes:
      - ${DATA_DIR:-./data}:/DATA
    depends_on:
      ollama:
        condition: service_healthy
    restart: on-failure
    networks:
      - app_network

  ollama:
    image: ollama/ollama:rocm
    container_name: ollama
    hostname: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_DEBUG=0
      - OLLAMA_ORIGINS=*
      - HSA_OVERRIDE_GFX_VERSION=11.0.0  # Adjust for your GPU
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render
    volumes:
      - ${OLLAMA_DATA_DIR:-./ollama-data}:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - app_network

networks:
  app_network:
    driver: bridge