# Both Backends - CPU
# Use this for maximum reliability on CPU: Ollama as primary, AnythingLLM as fallback
# Requirements: 8+ CPU cores, 16GB+ RAM, API key for fallback
# Warning: Ollama on CPU is VERY slow. Consider using AnythingLLM-only instead.

services:
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    hostname: anythingllm
    container_name: anythingllm
    ports: 
      - "3001:3001"
    environment:
      - STORAGE_DIR=/app/server/storage
      - DISABLE_TELEMETRY=true
    volumes:
      - ${ANYTHINGLLM_DATA_DIR:-./anythingllm-data}:/app/server/storage
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - app_network

  jobcollector:
    build:
      context: ../.
      dockerfile: Dockerfile
    image: drjp81/linkedinscrape:latest
    container_name: jobcollector
    environment:
      # Collector parameters
      - SITE_NAME=${SITE_NAME:-indeed,linkedin,glassdoor}
      - SEARCH_TERMS=${SEARCH_TERMS:-Azure,devops}
      - LOCATION=${LOCATION:-Canada}
      - RESULTS_WANTED=${RESULTS_WANTED:-20}
      - HOURS_OLD=${HOURS_OLD:-24}
      - COUNTRY_INDEED=${COUNTRY_INDEED:-canada}
      - LINKEDIN_FETCH_DESCRIPTION=${LINKEDIN_FETCH_DESCRIPTION:-true}
      - DATA_DIR=/DATA
      # Ollama parameters (primary, but slow on CPU)
      - OLLAMA_BASE=${OLLAMA_BASE:-http://ollama:11434}
      - SCORER_MODEL=${SCORER_MODEL:-llama3.2:3b-instruct-q6_K}  # Smaller model for CPU
      # - EMBED_MODEL=${EMBED_MODEL:-nomic-embed-text:v1.5}
      - CANDIDATE_NAME=${CANDIDATE_NAME:-John Doe}
      - CANDIDATE_RESUME_PATH=${CANDIDATE_RESUME_PATH:-/DATA/resume/resume.txt}
      - TEMPERATURE=${TEMPERATURE:-0.3}
      - CONTEXT_LENGTH=${CONTEXT_LENGTH:-8192}  # Reduced for CPU
      - MUST_HAVES=${MUST_HAVES}
      - NICE_TO_HAVES=${NICE_TO_HAVES}
      - EXCLUSIONS=${EXCLUSIONS}
      - LOCALE=${LOCALE}
      - LANG_PREF=${LANG_PREF:-either}
      - SENIORITY_TARGET=${SENIORITY_TARGET:-senior}
      - K_RESUME_SNIPPETS=${K_RESUME_SNIPPETS:-6}
      - RESUME_CHUNK_SIZE=${RESUME_CHUNK_SIZE:-900}
      - RESUME_CHUNK_OVERLAP=${RESUME_CHUNK_OVERLAP:-150}
      # AnythingLLM parameters (fallback - will likely be used)
      - ANLLM_API_KEY=${ANLLM_API_KEY}
      - ANLLM_API_WORKSPACE=${ANLLM_API_WORKSPACE:-job-searching}
      - ANLLM_API_BASE=${ANLLM_API_BASE:-http://anythingllm:3001}
      - RETRIES=${RETRIES:-3}
      - TIMEOUT_MINUTES=${TIMEOUT_MINUTES:-10}  # Longer timeout for CPU
    volumes:
      - ${DATA_DIR:-./data}:/DATA
    depends_on:
      ollama:
        condition: service_healthy
      anythingllm:
        condition: service_healthy
    restart: on-failure
    networks:
      - app_network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    hostname: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_DEBUG=0
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=1  # Limit parallelism on CPU
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      - ${OLLAMA_DATA_DIR:-./ollama-data}:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - app_network

networks:
  app_network:
    driver: bridge