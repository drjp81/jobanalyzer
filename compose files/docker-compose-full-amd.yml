# Both Backends - AMD GPU
# Use this for maximum reliability: Ollama as primary, AnythingLLM as fallback
# Requirements: AMD GPU with ROCm support, API key for fallback

services:
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    hostname: anythingllm
    container_name: anythingllm
    ports: 
      - "3001:3001"
    environment:
      - STORAGE_DIR=/app/server/storage
      - DISABLE_TELEMETRY=true
    volumes:
      - ${ANYTHINGLLM_DATA_DIR:-./anythingllm-data}:/app/server/storage
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - app_network

  jobcollector:
    build:
      context: ../.
      dockerfile: Dockerfile
    image: drjp81/linkedinscrape:latest
    container_name: jobcollector
    environment:
      # Collector parameters
      - SITE_NAME=${SITE_NAME:-indeed,linkedin,glassdoor}
      - SEARCH_TERMS=${SEARCH_TERMS:-Azure,devops}
      - LOCATION=${LOCATION:-Canada}
      - RESULTS_WANTED=${RESULTS_WANTED:-20}
      - HOURS_OLD=${HOURS_OLD:-24}
      - COUNTRY_INDEED=${COUNTRY_INDEED:-canada}
      - LINKEDIN_FETCH_DESCRIPTION=${LINKEDIN_FETCH_DESCRIPTION:-true}
      - DATA_DIR=/DATA
      # Ollama parameters (primary)
      - OLLAMA_BASE=${OLLAMA_BASE:-http://ollama:11434}
      - SCORER_MODEL=${SCORER_MODEL:-qwen2.5:7b-instruct}
      # - EMBED_MODEL=${EMBED_MODEL:-nomic-embed-text:v1.5}
      - CANDIDATE_NAME=${CANDIDATE_NAME:-John Doe}
      - CANDIDATE_RESUME_PATH=${CANDIDATE_RESUME_PATH:-/DATA/resume/resume.txt}
      - TEMPERATURE=${TEMPERATURE:-0.3}
      - CONTEXT_LENGTH=${CONTEXT_LENGTH:-16384}
      - MUST_HAVES=${MUST_HAVES}
      - NICE_TO_HAVES=${NICE_TO_HAVES}
      - EXCLUSIONS=${EXCLUSIONS}
      - LOCALE=${LOCALE}
      - LANG_PREF=${LANG_PREF:-either}
      - SENIORITY_TARGET=${SENIORITY_TARGET:-senior}
      - K_RESUME_SNIPPETS=${K_RESUME_SNIPPETS:-6}
      - RESUME_CHUNK_SIZE=${RESUME_CHUNK_SIZE:-900}
      - RESUME_CHUNK_OVERLAP=${RESUME_CHUNK_OVERLAP:-150}
      # AnythingLLM parameters (fallback)
      - ANLLM_API_KEY=${ANLLM_API_KEY}
      - ANLLM_API_WORKSPACE=${ANLLM_API_WORKSPACE:-job-searching}
      - ANLLM_API_BASE=${ANLLM_API_BASE:-http://anythingllm:3001}
      - RETRIES=${RETRIES:-3}
      - TIMEOUT_MINUTES=${TIMEOUT_MINUTES:-5}
    volumes:
      - ${DATA_DIR:-./data}:/DATA
    depends_on:
      ollama:
        condition: service_healthy
      anythingllm:
        condition: service_healthy
    restart: on-failure
    networks:
      - app_network

  ollama:
    image: ollama/ollama:rocm
    container_name: ollama
    hostname: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_DEBUG=0
      - OLLAMA_ORIGINS=*
      - HSA_OVERRIDE_GFX_VERSION=11.0.0  # Adjust for your GPU
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render
    volumes:
      - ${OLLAMA_DATA_DIR:-./ollama-data}:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - app_network

networks:
  app_network:
    driver: bridge